## Dragon Real Estate - Price Predictor



import sklearn



import matplotlib.pyplot as plt



import numpy as np



import pandas as pd



housing=pd.read_csv("data.csv")



housing.head()



housing.info()



housing['CHAS'].value_counts()



housing.describe()



housing.min()



%matplotlib inline
housing.hist(bins=,figsize=(20,15))


	## Train Test Spillting



def split_train_test(data,test_ratio):

  
   np.random.seed(42)

    
   shuffl=np.random.permutation(len(data))


   print(shuffl)

 
  test_set_size=int(len(data)*test_ratio)
   test_indices=shuffl[:test_set_size]

   train_indices=shuffl[test_set_size:]
return data.iloc[train_indices], data.iloc[test_indices]



import seaborn as sns



#import configparser



#import datetime



#import nltk
train_set,test_set = split_train_test(housing,0.2)



print(f"Rows in train set: {len(train_set)}\nRows in test set: {len(test_set)}\n")



from sklearn.model_selection import train_test_split,train_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)


print(train_set,"\n",test_set)



from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)


for train_index, test_index in split.split(housing,housing['CHAS']):


     strat_train_set=housing.loc[train_index]

     strat_test_set=housing.loc[test_index]
     

strat_test_set.info()



print(strat_test_set['CHAS'].value_counts())



print(strat_train_set['CHAS'].value_counts())

housing = strat_train_set.copy() 	

	#For a long data set not for a short data set



## Looking For Corelations



corr_matrix=housing.corr()


corr_matrix



corr_matrix['MEDV'].sort_values(ascending=False)  


from pandas.plotting import scatter_matrix


attributes=["MEDV", "RM","ZN","LSTAT"]


scatter_matrix(housing[attributes],figsize=(12,8))



housing.plot(kind="scatter",x="TAX",y="MEDV" , alpha=0.8)



## Trying Out Attributes combinations



housing["AGE/RM"]=housing["AGE"]/housing["RM"]



housing["AGE/RM"]



housing.head()



housing.plot(kind="scatter",x="AGE/RM",y="MEDV" , alpha=0.8)



#housing["NOX/RM"]=housing["NOX"]/housing["RM"]



#housing.plot(kind="scatter",x="NOX/RM",y="MEDV" , alpha=0.8)



housing["LSTAT/RM"]=housing["LSTAT"]/housing["RM"]


housing.plot(kind="scatter",x="LSTAT/RM",y="MEDV" , alpha=0.8)



housing= strat_train_set.drop("MEDV",axis=1)


housing_labels= strat_train_set["MEDV"].copy()



# Missing Attributes



#To Take Care Missing Attributes


#1. Get rid of the missing data points


#2. Get rid of the whole atttribute


#3. Set the value to some value(0,mean or median)



import sys



a=housing.dropna(subset=['RM']) 

#Option1


a.shape



housing.shape




median=housing["RM"].median()

# Option2


print(median)



#housing["RM"].fillna(median)
#Option3



mode=housing["RM"].mode()



print(mode)



mean=housing["RM"].mean()



print(mean)



from sklearn.impute import SimpleImputer


imputer=SimpleImputer(strategy="median")


imputer.fit(housing)



imputer.statistics_.shape



X=imputer.transform(housing)



housing_tr=pd.DataFrame(X,columns=housing.columns)



housing_tr.describe()



## Scikit-learn

## Design

Primarily, three types of objects

1. Estimators  - It estimates some parameter based on a dataset. Eg. imputer . It has a fit method and transform method. Fot method - Fits the dataset and calculates internal parameters.

2. Transformers - Transform method takes input and returns output based on the learning form fit(). It also has a convenience function called fit_transform() which fits and then transforms.

3. Predictors  -  Linear Regression model is an example of predictor. fit() and predict() are two common functions. It also gives score() function which will evaluate the predictions.



## Feature Scaling

Primarily, two types of features scaling methods:

1. Min-Max scaling    (Normalization)
   (value - min)/(max - min)
.
Sklearn provides a class called MinMaxScalar for this.

2. Standardization 
    (value - mean)/std(standard deviation)
.
Sklearn provides a class called StandardScalar for this.



## Creating a Pipeline



from sklearn.pipeline import Pipeline


from sklearn.preprocessing 
import StandardScaler


my_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),


#    ......... add as many you want in your pipeline
    
('std_scalar', StandardScaler()), 
])


housing_num_tr=my_pipeline.fit_transform(housing_tr)


housing_num_tr


housing_num_tr.shape


## Selecting a desired model for Dragon Real Estates


from sklearn.linear_model import LinearRegression

#
from sklearn.tree import DecisionTreeRegressor

#from sklearn.ensemble import RandomForestRegressor

model = LinearRegression()

#model = DecisionTreeRegressor()

#model = RandomForestRegressor()

model.fit(housing_num_tr,housing_labels)


some_data = housing.iloc[:5]


some_labels = housing_labels.iloc[:5]


prepared_data= my_pipeline.transform(some_data)

model.predict(prepared_data)




list(some_labels)


## Evaluating the Model


from sklearn.metrics import mean_squared_error

housing_predictions = model.predict(housing_num_tr)

lin_mse = mean_squared_error(housing_labels, housing_predictions)

lin_rmse=np.sqrt(lin_mse)


lin_rmse



## Using better evaluation technique - Cross Validation



from sklearn.model_selection import cross_val_score

scores = cross_val_score(model,housing_num_tr,housing_labels,scoring="neg_mean_squared_error", cv=10)

rmse_scores=np.sqrt(-scores)
rmse_scores


def print_scores(scores):

    print("Scores :",scores)

    print("Mean :",scores.mean())

    print("Standard Deviation :",scores.std())


print_scores(rmse_scores)


## Saving the Model


from joblib import dump, load

dump(model,"Dragon.joblib")


## Testing the model on test data
X_test= strat_test_set.drop("MEDV", axis=1)

Y_test = strat_test_set["MEDV"].copy()

X_test_prepared = my_pipeline.transform(X_test)
final_predictions = model.predict(X_test_prepared)

final_mse = mean_squared_error(Y_test, final_predictions)
final_rmse = np.sqrt(final_mse)

print(final_predictions,list(Y_test))
print(final_mse)
final_rmse


#prepared_data[0]


import numpy as np

features = np.array([[-0.43942006,  3.12628155, -1.12165014, -0.27288841, -0.24141041,
       -1.31238772,  3.61111401, -0.5778192 , -0.86091034]])
model.predict(features)